#Qwen/Qwen1.5-4B-Chat:
 # per_device_train_batch_size: 1
  #gradient_accumulation_steps: 16
  #num_train_epochs: 8
  #lora_rank: 8
  #lora_alpha: 32
  #lora_dropout: 0.5


# google/gemma-2b:
  # per_device_train_batch_size: 1
  # gradient_accumulation_steps: 8
  # num_train_epochs: 10
  # lora_rank: 4
  # lora_alpha: 8
  # lora_dropout: 0.1


microsoft/Phi-3.5-mini-instruct:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  num_train_epochs: 1
  lora_rank: 1
  lora_alpha: 1
  lora_dropout: 0.005

microsoft/Phi-3-small-8k-instruct:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  num_train_epochs: 8
  lora_rank: 8
  lora_alpha: 32
  lora_dropout: 0.5

microsoft/Phi-3-medium-4k-instruct:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 16
  num_train_epochs: 8
  lora_rank: 8
  lora_alpha: 32
  lora_dropout: 0.5
 
  
#Qwen/Qwen2.5-3B-Instruct:
 # per_device_train_batch_size: 1
  #gradient_accumulation_steps: 8
  #num_train_epochs: 10
  #lora_rank: 4
  #lora_alpha: 8
  #lora_dropout: 0.1
